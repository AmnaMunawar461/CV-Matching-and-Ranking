{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeca86bd-b69b-451c-95df-499809513aa6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaed257b-ac81-464a-ae48-638268295374",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import en_core_web_sm\n",
    "nlp=en_core_web_sm.load()\n",
    "import csv\n",
    "from pyresparser import ResumeParser\n",
    "\n",
    "from docx import Document\n",
    "import glob\n",
    "import os\n",
    "from curses import ascii\n",
    "def clean(text):\n",
    "    return str(''.join(\n",
    "            ascii.isprint(c) and c or ' ' for c in text\n",
    "            )) \n",
    "def conversion_word():\n",
    "            \n",
    "    for i in range(1,24):\n",
    "        file = 'C:/Users/HOME/CV/cvs/'+ str(i)+'.txt'\n",
    "        doc = Document()\n",
    "\n",
    "        with open(file, 'r', encoding='utf-8',errors='ignore') as openfile:\n",
    "    \n",
    "                line = openfile.read()\n",
    "                s=line.replace('\\ufeff----------------------- Page 1-----------------------\\n\\n','').replace('Page 1','').replace('-','').replace('Page 2','').replace('Page 3','').replace('\\n','')\n",
    "                s=clean(s)\n",
    "                doc.add_paragraph(s)\n",
    "                doc.save(file + \".docx\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ce8f95-ae47-48ac-a586-e51536e2ce0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "target=['BOTH','LI','RA','RA','LI','BOTH','RA','BOTH','LI','RA','BOTH','BOTH','BOTH','BOTH','BOTH','BOTH','BOTH','RA','BOTH','LI','LI','LI','BOTH']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 707,
   "id": "6e7fb73c-8746-4629-885f-1105d6cd2ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "lists=[]\n",
    "for i in range(1,24):\n",
    "    data=ResumeParser('C:/Users/Home/CV/cvs/'+str(i)+'.txt.docx').get_extracted_data()\n",
    "    for x in data['skills']:\n",
    "        if  x not in lists:\n",
    "            lists.append(x)\n",
    "list1=[]\n",
    "for i in range(1,24):\n",
    "    data=ResumeParser('C:/Users/Home/CV/cvs/'+str(i)+'.txt.docx').get_extracted_data()\n",
    "    list1.append(data['skills'])\n",
    "li = [[0 for i in range(len(lists))] for j in range(23)]\n",
    "for i in range(len(list1)):\n",
    "    for l in range(len(lists)):\n",
    "        if lists[l] in list1[i]:\n",
    "            li[i][l]=1\n",
    "        else:\n",
    "            li[i][l]=0\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "id": "7819aab0-e452-4ee6-ba22-8fb5b8fd2309",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "describe_=[]\n",
    "for i in range(1,3):\n",
    "    file = 'C:/Users/HOME/CV/cvs/JD '+ str(i)+'.txt'\n",
    "    with open(file, 'r', encoding='utf-8',errors='ignore') as openfile:    \n",
    "        line = openfile.read()\n",
    "        line=re.sub('[^A-Za-z]+', ' ', line)\n",
    "    describe_.append(line)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X = vectorizer.fit(describe_)\n",
    "d=vectorizer.vocabulary_\n",
    "vocab_=[]\n",
    "for i in d.keys():\n",
    "    vocab_.append(i)\n",
    "removes =['job','description','defined','work','experience','ases','uw','related','ways','typical','chair']\n",
    "for x in removes:\n",
    "    if  x in vocab_:\n",
    "        vocab_.remove(x)\n",
    "find = [[0 for i in range(len(vocab_))] for j in range(24)]\n",
    "   \n",
    "for i in range(1,24):\n",
    "    file = 'C:/Users/HOME/CV/cvs/'+ str(i)+'.txt'\n",
    "\n",
    "    with open(file, 'r', encoding='utf-8',errors='ignore') as openfile:\n",
    "    \n",
    "            line = openfile.read()\n",
    "    for v in range(len(vocab_)):\n",
    "        if vocab_[v] in line:\n",
    "                find[i-1][v]=1\n",
    "        else:\n",
    "                find[i-1][v]=0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021be214-d369-4a5f-b693-85047667fd94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4091b807-d3c0-4fd1-8cc8-05b81ae02055",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aecf7f59-77a8-4677-b107-ba0e56edf727",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lists' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-ab7319e47476>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mwriter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriterow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'name'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'email'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'mobile_number'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"datafile.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlists\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m     \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"datafile.csv\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'lists' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn import metrics \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import en_core_web_sm\n",
    "nlp=en_core_web_sm.load()\n",
    "import json\n",
    "f=open('C:/Users/HOME/CV/describe_job.json',)\n",
    "vocab_=json.load(f)\n",
    "f_=open('C:/Users/HOME/CV/skills.json',)\n",
    "lists=json.load(f_)\n",
    "\n",
    "import csv\n",
    "with open('datafile.csv', 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['name','email','mobile_number'])\n",
    "df = pd.read_csv(\"datafile.csv\")\n",
    "for l in lists:\n",
    "    df[l]=''\n",
    "df.to_csv(\"datafile.csv\", index=False)\n",
    "df = pd.read_csv(\"datafile.csv\")\n",
    "for vocab in vocab_:\n",
    "    df[vocab]=''\n",
    "df['target']=''\n",
    "df.to_csv(\"datafile.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "id": "b7d0375f-921a-4be6-97f5-6fd45ca190ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('datafile.csv', 'a', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    for i in range(1,24):\n",
    "        data=ResumeParser('C:/Users/Home/CV/cvs/'+str(i)+'.txt.docx').get_extracted_data()\n",
    "        \n",
    "        writer.writerow([data['name'],data['email'],data['mobile_number'],\n",
    "                         li[i-1][0],li[i-1][1],li[i-1][2],li[i-1][3],li[i-1][4],li[i-1][5],li[i-1][6],li[i-1][7],li[i-1][8],li[i-1][9],li[i-1][10],li[i-1][11],li[i-1][12],li[i-1][13],li[i-1][14],li[i-1][15],li[i-1][16],li[i-1][17],li[i-1][18],li[i-1][19],li[i-1][20],\n",
    "                         li[i-1][21],li[i-1][22],li[i-1][23],li[i-1][24],li[i-1][25],li[i-1][26],li[i-1][27],li[i-1][28],li[i-1][29],li[i-1][30],li[i-1][31],li[i-1][32],li[i-1][33],li[i-1][34],\n",
    "                         li[i-1][35],li[i-1][36],li[i-1][37],li[i-1][38],li[i-1][39],li[i-1][40],li[i-1][41],li[i-1][42],\n",
    "                         li[i-1][43],li[i-1][44],li[i-1][45],li[i-1][46],li[i-1][47],li[i-1][48],li[i-1][49],li[i-1][50],\n",
    "                         li[i-1][51],li[i-1][52],li[i-1][53],li[i-1][54],li[i-1][55],li[i-1][56],li[i-1][57],li[i-1][58],\n",
    "                         li[i-1][59],li[i-1][60],li[i-1][61],li[i-1][62],li[i-1][63],li[i-1][64],li[i-1][65],li[i-1][66],\n",
    "                         li[i-1][67],li[i-1][68],li[i-1][69],li[i-1][70],li[i-1][71],li[i-1][72],li[i-1][73],li[i-1][74],\n",
    "                         li[i-1][75],li[i-1][76],li[i-1][77],li[i-1][78],li[i-1][79],li[i-1][80],li[i-1][81],li[i-1][82],\n",
    "                         li[i-1][83],li[i-1][84],li[i-1][85],li[i-1][86],li[i-1][87],li[i-1][88],li[i-1][89],li[i-1][90],\n",
    "                         li[i-1][91],li[i-1][92],li[i-1][93],li[i-1][94],li[i-1][95],li[i-1][96],li[i-1][97],li[i-1][98],\n",
    "                         li[i-1][99],li[i-1][100],li[i-1][101],li[i-1][102],li[i-1][103],li[i-1][104],li[i-1][105],li[i-1][106],\n",
    "                         li[i-1][107],li[i-1][108],li[i-1][109],li[i-1][110],li[i-1][111],li[i-1][112],li[i-1][113],li[i-1][114],\n",
    "                         li[i-1][115],li[i-1][116],li[i-1][117],li[i-1][118],li[i-1][119],li[i-1][120],li[i-1][121],li[i-1][122],\n",
    "                         li[i-1][123],li[i-1][124],li[i-1][125],li[i-1][126],li[i-1][127],li[i-1][128],li[i-1][129],li[i-1][130],\n",
    "                         li[i-1][131],li[i-1][132],li[i-1][133],li[i-1][134],li[i-1][135],li[i-1][136],li[i-1][137],li[i-1][138],\n",
    "                         li[i-1][139],li[i-1][140],li[i-1][141],li[i-1][142],li[i-1][143],li[i-1][144],li[i-1][145],li[i-1][146],\n",
    "                         li[i-1][147],li[i-1][148],li[i-1][149],\n",
    "                         li[i-1][150],li[i-1][151],li[i-1][152],li[i-1][153],li[i-1][154],li[i-1][155],li[i-1][156],li[i-1][157],\n",
    "                         li[i-1][158],li[i-1][159],li[i-1][160],li[i-1][161],li[i-1][162],\n",
    "                         find[i-1][0],find[i-1][1],find[i-1][2],find[i-1][3],find[i-1][4],find[i-1][5],find[i-1][6],find[i-1][7],find[i-1][8],find[i-1][9],find[i-1][10],find[i-1][11],find[i-1][12],find[i-1][13],find[i-1][14],find[i-1][15],find[i-1][16],find[i-1][17],find[i-1][18],find[i-1][19],find[i-1][20],\n",
    "                         find[i-1][21],find[i-1][22],find[i-1][23],find[i-1][24],find[i-1][25],find[i-1][26],find[i-1][27],find[i-1][28],find[i-1][29],find[i-1][30],find[i-1][31],find[i-1][32],find[i-1][33],find[i-1][34],\n",
    "                         find[i-1][35],find[i-1][36],find[i-1][37],find[i-1][38],find[i-1][39],find[i-1][40],find[i-1][41],find[i-1][42],\n",
    "                         find[i-1][43],find[i-1][44],find[i-1][45],find[i-1][46],find[i-1][47],find[i-1][48],find[i-1][49],find[i-1][50],\n",
    "                         find[i-1][51],find[i-1][52],find[i-1][53],find[i-1][54],find[i-1][55],find[i-1][56],find[i-1][57],find[i-1][58],\n",
    "                         find[i-1][59],find[i-1][60],find[i-1][61],find[i-1][62],find[i-1][63],find[i-1][64],find[i-1][65],find[i-1][66],\n",
    "                         find[i-1][67],find[i-1][68],find[i-1][69],find[i-1][70],find[i-1][71],find[i-1][72],find[i-1][73],find[i-1][74],\n",
    "                         find[i-1][75],find[i-1][76],find[i-1][77],find[i-1][78],find[i-1][79],find[i-1][80],find[i-1][81],find[i-1][82],\n",
    "                         find[i-1][83],find[i-1][84],find[i-1][85],find[i-1][86],find[i-1][87],find[i-1][88],find[i-1][89],find[i-1][90],\n",
    "                         find[i-1][91],find[i-1][92],find[i-1][93],find[i-1][94],find[i-1][95],find[i-1][96],find[i-1][97],find[i-1][98],\n",
    "                         find[i-1][99],find[i-1][100],find[i-1][101],find[i-1][102],find[i-1][103],find[i-1][104],find[i-1][105],find[i-1][106],\n",
    "                         find[i-1][107],find[i-1][108],find[i-1][109],find[i-1][110],find[i-1][111],find[i-1][112],find[i-1][113],find[i-1][114],\n",
    "                         find[i-1][115],find[i-1][116],find[i-1][117],find[i-1][118],find[i-1][119],find[i-1][120],find[i-1][121],find[i-1][122],\n",
    "                         find[i-1][123],find[i-1][124],find[i-1][125],find[i-1][126],find[i-1][127],find[i-1][128],find[i-1][129],find[i-1][130],\n",
    "                         find[i-1][131],find[i-1][132],find[i-1][133],find[i-1][134],find[i-1][135],find[i-1][136],find[i-1][137],find[i-1][138],\n",
    "                         find[i-1][139],find[i-1][140],find[i-1][141],find[i-1][142],find[i-1][143],find[i-1][144],find[i-1][145],find[i-1][146],\n",
    "                         find[i-1][147],find[i-1][148],find[i-1][149],\n",
    "                         find[i-1][150],find[i-1][151],find[i-1][152],find[i-1][153],find[i-1][154],find[i-1][155],find[i-1][156],find[i-1][157],\n",
    "                         find[i-1][158],find[i-1][159],find[i-1][160],find[i-1][161],find[i-1][162],find[i-1][163],find[i-1][164],find[i-1][165],\n",
    "                         find[i-1][166],find[i-1][167],find[i-1][168],find[i-1][169],find[i-1][170],find[i-1][171],find[i-1][172],find[i-1][173],\n",
    "                         find[i-1][174],find[i-1][175],find[i-1][176],find[i-1][177],find[i-1][178],find[i-1][179],find[i-1][180],find[i-1][181],\n",
    "                         find[i-1][182],find[i-1][183],find[i-1][184],find[i-1][185],find[i-1][186],find[i-1][187],find[i-1][188],find[i-1][189],\n",
    "                         find[i-1][190],find[i-1][191],find[i-1][192],find[i-1][193],find[i-1][194],find[i-1][195],find[i-1][196],find[i-1][197],\n",
    "                         find[i-1][198],find[i-1][199],find[i-1][200],find[i-1][201],find[i-1][202],find[i-1][203],find[i-1][204],find[i-1][205],\n",
    "                         find[i-1][206],find[i-1][207],find[i-1][208],find[i-1][209],find[i-1][210],find[i-1][211],find[i-1][212],find[i-1][213],\n",
    "                         find[i-1][214],find[i-1][215],find[i-1][216],find[i-1][217],find[i-1][218],find[i-1][219],find[i-1][220],find[i-1][221],\n",
    "                         find[i-1][222],find[i-1][223],find[i-1][224],find[i-1][225],find[i-1][226],find[i-1][227],find[i-1][228],find[i-1][229],\n",
    "                         find[i-1][230],find[i-1][231],find[i-1][232],find[i-1][233],find[i-1][234],find[i-1][235],find[i-1][236],find[i-1][237],\n",
    "                         find[i-1][238],find[i-1][239],find[i-1][240],find[i-1][241],find[i-1][242],find[i-1][243],find[i-1][244],find[i-1][245],\n",
    "                         find[i-1][246],find[i-1][247],find[i-1][248],find[i-1][249],find[i-1][250],find[i-1][251],find[i-1][252],find[i-1][253],\n",
    "                         find[i-1][254],find[i-1][255],find[i-1][256],find[i-1][257],find[i-1][258],find[i-1][259],find[i-1][260],find[i-1][261],\n",
    "                         find[i-1][262],target[i-1]])\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "id": "0c3e050b-d3cf-4d44-83bd-ebfeeae79229",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'S'"
      ]
     },
     "execution_count": 615,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65beb22c-5523-4134-8788-e1130e56bb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import metrics \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "id": "b5945932-6a69-4df3-8cbf-3f8beb32ef15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 620,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('datafile.csv')\n",
    "df.fillna('')\n",
    "df.head()\n",
    "row=df.axes[0]\n",
    "col=df.axes[1]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "id": "a817a3d3-91dd-4b0f-b0db-5bdc22820c50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "263"
      ]
     },
     "execution_count": 627,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = df.iloc[:, 2:-1].values\n",
    "y = df.iloc[:, len(col)-1].values\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983a99fb-8d94-4a8c-8aba-60c9c2479fcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 688,
   "id": "410f2fc9-3a89-45ac-8a0a-ef305043c5b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BOTH' 'LI' 'RA' 'RA' 'LI' 'BOTH' 'RA' 'BOTH' 'LI' 'RA' 'BOTH' 'BOTH'\n",
      " 'BOTH' 'BOTH' 'BOTH' 'BOTH' 'BOTH' 'RA' 'BOTH' 'LI' 'LI' 'LI' 'BOTH'] [0 1 2 2 1 0 2 0 1 2 0 0 0 0 0 0 0 2 0 1 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "le=preprocessing.LabelEncoder()\n",
    "y1=le.fit_transform(y)\n",
    "y_classes=le.inverse_transform(y1)\n",
    "print(y_classes,y1)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y1, random_state=10)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifier = RandomForestClassifier(n_estimators = 40)\n",
    "classifier.fit(x_train, y_train)\n",
    "y_pred = classifier.predict(x_test) # test the output by changing values\n",
    "#classifier.predict_proba(x_test)\n",
    "classifier.score(x_test,y_test)\n",
    "#print(y_pred,y_test)\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "result = confusion_matrix(y_test, y_pred)\n",
    "#print(\"Confusion Matrix:\")\n",
    "#print(result)\n",
    "#result1 = classification_report(y_test, y_pred)\n",
    "#print(\"Classification Report:\",)\n",
    "#print (result1)\n",
    "#result2 = accuracy_score(y_test,y_pred)\n",
    "#print(\"Accuracy:\",result2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "id": "3a45eb4c-f3b7-4338-bba9-3fb9f9f37d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename = 'finalized_model.sav'\n",
    "pickle.dump(classifier, open(filename, 'wb'))\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "id": "c352a428-9df4-4a5a-bfd3-984485c71bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "result = loaded_model.score(x_test, y_test)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 695,
   "id": "3dc0053a-0013-43a2-abd3-8b385afbcd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "li = [[0 for i in range(len(lists))] for j in range(23)]\n",
    "for i in range(len(list1)):\n",
    "    for l in range(len(lists)):\n",
    "        if lists[l] in list1[i]:\n",
    "            li[i][l]=1\n",
    "        else:\n",
    "            li[i][l]=0\n",
    "find = [[0 for i in range(len(vocab_))] for j in range(23)]\n",
    "   \n",
    "for i in range(1,24):\n",
    "    file = 'C:/Users/HOME/CV/cvs/'+ str(i)+'.txt'\n",
    "\n",
    "    with open(file, 'r', encoding='utf-8',errors='ignore') as openfile:\n",
    "    \n",
    "            line = openfile.read()\n",
    "    for v in range(len(vocab_)):\n",
    "        if vocab_[v] in line:\n",
    "                find[i-1][v]=1\n",
    "        else:\n",
    "                find[i-1][v]=0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 697,
   "id": "a5e07702-4416-449d-a91b-9e4cf19de0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cv.csv', 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['name','email','mobile_number'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212d81d9-dd3f-4b92-8515-9d7758440e91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 698,
   "id": "5c7249a3-b2b3-4b69-9f45-f31892c8fdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv(\"cv.csv\")\n",
    "for l in lists:\n",
    "    df[l]=''\n",
    "for vocab in vocab_:\n",
    "    df[vocab]=''\n",
    "df.to_csv(\"cv.csv\", index=False)\n",
    "with open('cv.csv', 'a', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    data=ResumeParser('C:/Users/Home/CV/cvs/'+str(11)+'.txt.docx').get_extracted_data()\n",
    "    writer.writerow([data['name'],data['email'],data['mobile_number'],\n",
    "li[0],li[1],li[2],li[3],li[4],li[5],li[6],li[7],li[8],li[9],li[10],li[11],li[12],li[13],li[14],li[15],li[16],li[17],li[18],li[19],\n",
    "li[20],\n",
    "                         li[21],li[22],li[23],li[24],li[25],li[26],li[27],li[28],li[29],li[30],li[31],li[32],li[33],li[34],\n",
    "                         li[35],li[36],li[37],li[38],li[39],li[40],li[41],li[42],\n",
    "                         li[43],li[44],li[45],li[46],li[47],li[48],li[49],li[50],\n",
    "                         li[51],li[52],li[53],li[54],li[55],li[56],li[57],li[58],\n",
    "                         li[59],li[60],li[61],li[62],li[63],li[64],li[65],li[66],\n",
    "                         li[67],li[68],li[69],li[70],li[71],li[72],li[73],li[74],\n",
    "                         li[75],li[76],li[77],li[78],li[79],li[80],li[81],li[82],\n",
    "                         li[83],li[84],li[85],li[86],li[87],li[88],li[89],li[90],\n",
    "                         li[91],li[92],li[93],li[94],li[95],li[96],li[97],li[98],\n",
    "                         li[99],li[100],li[101],li[102],li[103],li[104],li[105],li[106],\n",
    "                         li[107],li[108],li[109],li[110],li[111],li[112],li[113],li[114],\n",
    "                         li[115],li[116],li[117],li[118],li[119],li[120],li[121],li[122],\n",
    "                         li[123],li[124],li[125],li[126],li[127],li[128],li[129],li[130],\n",
    "                    li[131],li[132],li[133],li[134],li[135],li[136],li[137],li[138],\n",
    "                         li[139],li[140],li[141],li[142],li[143],li[144],li[145],li[146],\n",
    "                         li[147],li[148],li[149],\n",
    "                         li[150],li[151],li[152],li[153],li[154],li[155],li[156],li[157],\n",
    "                         li[158],li[159],li[160],li[161],li[162],\n",
    "                         find[0],find[1],find[2],find[3],find[4],find[5],find[6],find[7],find[8],find[9],find[10],find[11],\n",
    "                        find[12],find[13],find[14],find[15],find[16],find[17],find[18],find[19],find[20],\n",
    "                         find[21],find[22],find[23],find[24],find[25],find[26],find[27],find[28],find[29],find[30],\n",
    "                                      find[31],find[32],find[33],find[34],\n",
    "                         find[35],find[36],find[37],find[38],find[39],find[40],find[41],find[42],\n",
    "                         find[43],find[44],find[45],find[46],find[47],find[48],find[49],find[50],\n",
    "                         find[51],find[52],find[53],find[54],find[55],find[56],find[57],find[58],\n",
    "                         find[59],find[60],find[61],find[62],find[63],find[64],find[65],find[66],\n",
    "                         find[67],find[68],find[69],find[70],find[71],find[72],find[73],find[74],\n",
    "                         find[75],find[76],find[77],find[78],find[79],find[80],find[81],find[82],\n",
    "                         find[83],find[84],find[85],find[86],find[87],find[88],find[89],find[90],\n",
    "                         find[91],find[92],find[93],find[94],find[95],find[96],find[97],find[98],\n",
    "                         find[99],find[100],find[101],find[102],find[103],find[104],find[105],find[106],\n",
    "                         find[107],find[108],find[109],find[110],find[111],find[112],find[113],find[114],\n",
    "                         find[115],find[116],find[117],find[118],find[119],find[120],find[121],find[122],\n",
    "                         find[123],find[124],find[125],find[126],find[127],find[128],find[129],find[130],\n",
    "                         find[131],find[132],find[133],find[134],find[135],find[136],find[137],find[138],\n",
    "                         find[139],find[140],find[141],find[142],find[143],find[144],find[145],find[146],\n",
    "                         find[147],find[148],find[149],\n",
    "                         find[150],find[151],find[152],find[153],find[154],find[155],find[156],find[157],\n",
    "                         find[158],find[159],find[160],find[161],find[162],find[163],find[164],find[165],\n",
    "                         find[166],find[167],find[168],find[169],find[170],find[171],find[172],find[173],\n",
    "                         find[174],find[175],find[176],find[177],find[178],find[179],find[180],find[181],\n",
    "                         find[182],find[183],find[184],find[185],find[186],find[187],find[188],find[189],\n",
    "                         find[190],find[191],find[192],find[193],find[194],find[195],find[196],find[197],\n",
    "                         find[198],find[199],find[200],find[201],find[202],find[203],find[204],find[205],\n",
    "                         find[206],find[207],find[208],find[209],find[210],find[211],find[212],find[213],\n",
    "                         find[214],find[215],find[216],find[217],find[218],find[219],find[220],find[221],\n",
    "                         find[222],find[223],find[224],find[225],find[226],find[227],find[228],find[229],\n",
    "                         find[230],find[231],find[232],find[233],find[234],find[235],find[236],find[237],\n",
    "                         find[238],find[239],find[240],find[241],find[242],find[243],find[244],find[245],\n",
    "                         find[246],find[247],find[248],find[249],find[250],find[251],find[252],find[253],\n",
    "                         find[254],find[255],find[256],find[257],find[258],find[259],find[260],find[261],\n",
    "                         find[262]\n",
    "                                           ])\n",
    "                                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 699,
   "id": "ed5d16a3-d892-4ea2-91a3-d36e3e5fba71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification():\n",
    "    df = pd.read_csv('cv.csv')\n",
    "    x = df.iloc[:, 2:].values\n",
    "    #print(x)\n",
    "    #classifier.predict_proba(x_test)\n",
    "    ynew = loaded_model.predict(x)\n",
    "# show the inputs and predicted probabilities\n",
    "    if ynew == 1:\n",
    "        print('Class is ',ynew[0])\n",
    "    elif ynew == 2:\n",
    "        print('Class is ',ynew[0])\n",
    "    else:\n",
    "        print('Class is ',ynew[0])\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef39177-e0a6-467f-82fb-70ec1926bca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_cv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 700,
   "id": "3023ab3b-8ebf-4e24-ab61-495338333441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class is  0\n"
     ]
    }
   ],
   "source": [
    "classification()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ced0ffc-556c-47ba-992a-447478ffec28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# o for both ,1 for li ,2 Ra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 703,
   "id": "a2cfbfa9-44f5-4c26-b9cc-9cc15af59afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "out_file = open(\"describe_job.json\", \"w\")\n",
    "  \n",
    "json.dump(vocab_, out_file)\n",
    "  \n",
    "out_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 706,
   "id": "29783c37-6f5e-4fb5-8bee-dd17a6c50834",
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open('skills.json','w')\n",
    "json.dump(lists,f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4658ecd-1a78-4945-9fd9-f92f598a10ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
